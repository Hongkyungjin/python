{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 때때로 온갖 종류의 텍스트 문서에서 원천 데이터를 확보한다. \n",
    "\n",
    "텍스트 문서는 정형 문서(HTML, XML, CSV, JSON 파일)와 비정형 문서(플레인 텍스트, 사람이 읽을 수 있는 텍스트)로 나눈다. \n",
    "\n",
    "비정형 데이터는 프로세싱 소프트웨어가 데이터 의미를 추론해야 하므로 가장 다루기 힘든 데이터 소스로 꼽힌다.\n",
    " \n",
    "앞에서 언급한 데이터는 모두 사람이 읽을 수 있는 형태다. \n",
    "\n",
    "필요하다면 텍스트 편집기(윈도에서는 Notepad, 리눅스에서는 gedit, 맥 OS X에서는 TextEdit)를 사용해서\n",
    "\n",
    "텍스트 파일을 열어 눈으로 읽고 손으로 편집할 수 있다. \n",
    "\n",
    "다른 도구를 사용할 수 없는 상황일 때는 표현 구조에 관계없이 텍스트 문서를 텍스트로 취급하고, \n",
    "\n",
    "파이썬의 문자열 함수를 사용해서 내용을 살펴보면 된다.\n",
    " \n",
    "운이 좋게도 아나콘다는 이를 해결할 수 있는 몇 가지 훌륭한 모듈을 제공한다. \n",
    "\n",
    "BeautifulSoup, csv, json, nltk는 두렵게만 보이는 텍스트 분석을 아주 흥미롭게 만들어 준다. \n",
    "\n",
    "“실체가 필요 이상으로 늘어나서는 안 된다.”는 오컴의 면도날(Occam’s razor) 원칙에 따라, \n",
    "\n",
    "우리는 이미 있는 도구를 다시 새로 만들지 말아야 한다. \n",
    "\n",
    "이는 텍스트 처리 도구뿐만 아니라 아나콘다 패키지에도 해당한다.\n",
    " \n",
    "가장 간단한 정형 데이터의 사례로 ‘텍스트 데이터 다루기’를 시작해 보자. \n",
    "\n",
    "그리고 자연어 처리 기법을 사용해서 비정형 데이터를 구조화하는 방법도 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTML 파일 처리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자주 쓰이는 HTML 태그와 속성\n",
    "\n",
    "|  태그 |  속성 |  목적 |\n",
    "| :--- | :---: | ---: |\n",
    "|  HTML | .  |  HTML 문서 전체 |\n",
    "|  HEAD | .  |  문서 헤더 |\n",
    "|  TITLE |  . |  문서 제목 |\n",
    "|  BODY |  background, bgcolor |  문서 바디 |\n",
    "|  H1, H2, H3 등 | .  |  섹션 헤더 |\n",
    "|  I, EM | .  |  강조-이탤릭 |\n",
    "|  B, STRONG | .  |  강조-볼드 |\n",
    "|  PRE |  . |  미리 설정된 포맷 |\n",
    "|  P, SPAN, DIV |  . |  문단, span, division |\n",
    "|  BR |  . |  줄 바꿈 |\n",
    "|  A |  href |  하이퍼링크 |\n",
    "|  IMG |  src, width, height |  이미지 |\n",
    "|  TABLE |  width, border |  테이블 |\n",
    "|  TR |  . |  테이블 행 |\n",
    "|  TH, TD | .  |  테이블 헤더, 데이터 셀 |\n",
    "|  OL, UL |  . |  순번, 비순번 리스트 |\n",
    "|  LI |  . |  리스트 아이템 |\n",
    "|  DL | .  |  서술 리스트 |\n",
    "|  DT, DD | .  |  서술 주제, 정의 |\n",
    "|  INPUT |  name |  사용자 입력 필드 |\n",
    "|  SELECT |  name |  풀다운 메뉴 |\n",
    "\n",
    "## XML? HTML?\n",
    "\n",
    "XML과 HTML은 외형적으로 비슷하지만, HTML 문서는 유효한 XML 문서가 아니다. \n",
    "\n",
    "XML 문서도 역시 HTML 문서가 아니다.\n",
    "\n",
    "XML 태그는 사용처에 따라 다르다. \n",
    "\n",
    "산형괄호로 둘러싸는 등 몇 가지 규칙만 지킨다면 알파벳이나 숫자로 된 문자열도 태그가 될 수 있다. \n",
    "\n",
    "XML 태그는 텍스트가 표현되는 방식은 다룰 수 없고, 그 해석(interpretation)만 다룰 수 있다. \n",
    "\n",
    "XML은 사람이 직접 읽지 않는 문서에 주로 사용한다. \n",
    "\n",
    "또 다른 언어인 XSLT(eXtensible Stylesheet Language Transformation)는 XML을 HTML로 바꾸고, \n",
    "\n",
    "CSS(Cascading Style Sheets)는 HTML 문서에 스타일을 더한다.\n",
    "\n",
    "## BeautifulSoup 모듈 다루기 (1)\n",
    "\n",
    "BeautifulSoup 모듈은 HTML과 XML 문서를 파싱하고 읽고 변형하는 데 사용한다. \n",
    "\n",
    "마크업 문자열, 마크업 파일, 웹에 있는 마크업 문서에 \n",
    "\n",
    "연결된 URL에서 BeautifulSoup 객체를 생성할 수 있다. \n",
    "\n",
    "BeautifulSoup4를 설치하지 않았다면 \n",
    "\n",
    "`conda install BeautifulSoup4` 명령어로 설치한다.\n",
    "\n",
    "soup을 준비했다면, `soup.prettify()` 함수로 마크업 문서를 읽기 쉬운 형태로 출력할 수 있다.\n",
    "\n",
    "`soup.get_text()` 함수는 마크업 문서에서 모든 태그를 제거하고 텍스트 부분만 반환한다. \n",
    "\n",
    "텍스트만 출력하고 싶다면 이 함수를 사용해서 마크업 문서를 플레인 텍스트로 변환할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "My document\n",
      "Main text.\n",
      "\n",
      "\n",
      "\n",
      "My Little Network Science Lab\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "My Little Network Science Lab\n",
      "By Dmitry Zinoviev\n",
      "\n",
      "Books\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I am excited to announce my books, \"Data Science Essentials in Python. Collect →  Organize →  Explore →  Predict →  Value\" (a.k.a. DZPYDS) and \"Complex Network Analysis in Python. Recognize → Construct → Visualize → Analyze → Interpret\" (a.k.a. DZCNAPY), published by the Pragmatic Bookshelf.\n",
      "\n",
      "The first book is intended for seasoned data scientists and statisticians migrating from R to Python, as well as for beginners willing to learn elements of data science in Python.\n",
      "\n",
      "The book leads you from messy, unstructured artifacts stored in SQL and NoSQL databases to a neat, well-organized dataset. It covers text mining, machine learning, and network analysis; processing numeric data with the NumPy and Pandas modules; and describing and analyzing data using statistical and network-theoretical methods. It has actual examples of data analysis at work, as well as mini-projects for you to enjoy. \n",
      "\n",
      "  The second book shows how, starting with simple networks, one can convert real-life and synthetic network graphs into Networkx data structures. The reader will look at more sophisticated networks and learn more powerful machinery to handle centrality calculation, blockmodeling, and clique and community detection. Get familiar with presentation-quality network visualization tools, both programmable and interactive--such as Gephi, a CNA explorer. The reader will  adapt the patterns from the case studies to your problems, explore big networks with NetworKit, a high-performance networkx substitute. Each part in the book gives an overview of a class of networks, includes a practical study of networkx functions and techniques, and concludes with case studies from various fields, including social networking, anthropology, marketing, and sports analytics.\n",
      "\n",
      "  \n",
      "The books are available for purchase at the publisher's site, and on Amazon (DZPYDS, DZCNAPY).\n",
      "\n",
      "Presentations\n",
      "\n",
      "Networks of Music Groups as Success Predictors - Social networks, digital humanities\n",
      "Network Science Workshop - General networks\n",
      "Resilience in Transaction-Oriented Networks - Communication networks\n",
      "Peer Ratings in Massive Online Social Networks - Social networks\n",
      "Semantic Networks of Interests in Online NSSI Communities - Semantic networks\n",
      "Towards an Ideal Store - Product networks\n",
      "\n",
      "Publications\n",
      "\n",
      "D.Zinoviev, \"Analyzing Cultural Domains with Python,\" PragPub, 82, pp. 26-33, Apr 2016\n",
      "D. Zinoviev, D. Stefanescu, G. Fireman, and L. Swenson, \"Semantic networks of interests in online non-suicidal self-injury communities,\" Digital Health, doi:10.1177/2055207616642118, SAGE, Apr 2016\n",
      "D.Zinoviev, \"The Pain of Complexity,\", Leonardo, 2016\n",
      "D.Zinoviev, Z.Zhu, and K.Li, \"Building mini-categories in product networks,\" in Studies in Computational Intelligence, vol. 597, pp. 179-190, Springer, Mar 2015\n",
      "D.Zinoviev, H.Benbrahim, G.Meszoely, and D.Stefanescu, \"Mitigation of delayed management costs in transaction-oriented systems,\" Sep 2014\n",
      "D.Zinoviev, H.Benbrahim, G.Meszoely, and D.Stefanescu, \"Simulating resilience in transaction-oriented networks,\" in Proc. Spring Simulation Multi-Conference, (San Diego, CA), Apr. 2013\n",
      "D.Zinoviev, D.Stefanescu, L.Swenson, and G.Fireman, \"Semantic networks of interests in online NSSI communities,\" in Proc. Workshop \"Words and Networks\" (Evanston, IL), June 2012\n",
      "D.Zinoviev and S.Llewelyn, \"Co-Evolution of Friendship and Publishing in Online Blogging Social Networks,\" WebSci-2012 (poster)\n",
      "D.Zinoviev, \"Information diffusion in social networks,\" in Social Networking and Community Behavior Modeling: Qualitative and Quantitative Measures (M. Safar, ed.), Hershey, PA: IGI Global, Dec. 2011\n",
      "D.Zinoviev and V.Duong, \"A game theoretical approach to broadcast  information diffusion in social networks,\" 6. in Proc. 44th Annual Simulation Symp., (Boston, MA), pp. 47-52, Apr. 2011\n",
      "D.Zinoviev and V.Duong, \"A game theoretical approach to modeling full-duplex information dissemination,\" in Proc. Summer Simulation Multi-Conference, (Ottawa, Canada), pp. 358-363, July 2010\n",
      "D.Zinoviev, V.Duong, and H.Zhang, \"A game theoretical approach to modeling information dissemination in social networks,\" in Proc. Int. Multi-Conference on Complexity, Informatics and Cybernetics, vol. I, pp. 407-412, IIIS, Apr. 2010\n",
      "D.Zinoviev and V.Duong, \"Toward Understanding Friendship in Online Social Networks,\" The Intl J. of Technology, Knowledge, and Society, pp. 1-8, May 2009\n",
      "D.Zinoviev, \"Topology and Geometry of Online Social Networks,\" in Proc. 12th World Multi-Conference on Systemics, Cybernetics and Informatics, vol. VI, pp. 138-143, 2008\n",
      "\n",
      "Other Projects\n",
      "\n",
      "Vixi: The Game of Meaning, produced in collaboration with Evgenia Cherkasova from Philosophy Department.\n",
      "All Characters from War and Peace by L.Tolstoy\n",
      "Mapping the Bible: Social Networks in the Holy Book. A book of graphs.\n",
      "FIFA World Cup 2014: Who Beat Whom?\n",
      "The seed post \"9 American habits I lost when I moved to Germany\" and its 125 \"likes\" and \"shares\" on Facebook. Nodes represent Facebook users, node sizes - number of friends/followers. Two nodes are connected if they are friends/followers and both reacted to the seed post. Red nodes denote shares. The post was originally published at the yellow node.\n",
      "\n",
      "Contacts\n",
      "\n",
      "Email\n",
      "Suffolk University\n",
      "Google Scholar\n",
      "LinkedIn\n",
      "Academia.edu\n",
      "ResearchGate\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# 문자열에서 soup을 생성한다.\n",
    "soup1 = BeautifulSoup(''' <HTML>\n",
    "  <HEAD><TITLE>My document</TITLE></HEAD>\n",
    "  <BODY>Main text.</BODY></HTML>\n",
    "''')\n",
    "\n",
    "# 웹 문서에서 soup을 생성한다.\n",
    "# urlopen()이 \"http://\"를 자동으로 추가하지 않는다는 것을 기억하자!\n",
    "soup2 = BeautifulSoup(urlopen(\"http://www.networksciencelab.com/\"))\n",
    "\n",
    "print(soup1.get_text())\n",
    "print(soup2.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마크업 태그는 파일에서 특정 부분을 찾는 데 사용하기도 한다. \n",
    "\n",
    "예를 들어 여러분이 첫 번째 테이블 첫 번째 행에 관심이 있다고 하자. \n",
    "\n",
    "플레인 텍스트만으로는 원하는 목적을 달성하기 어려운데, \n",
    "\n",
    "class나 id 속성이 부여되었다면 태그로는 가능하다.\n",
    "\n",
    "BeautifulSoup은 태그 간 모든 상하적이고 수평적인 관계에서 일관된 접근 방식을 사용한다. \n",
    "\n",
    "태그 간 관계는 태그 객체의 속성으로 표현하며, 파일 시스템의 상하 구조와 유사하다. \n",
    "\n",
    "soup 제목인 `soup.title`은 soup 객체의 속성이다. \n",
    "\n",
    "제목에 있는 부모 엘리먼트(element)의 name 값은 `soup.title.parent.name.string`으로, \n",
    "\n",
    "첫 번째 테이블 첫 번째 행 첫 번째 셀은 `soup.body.table.tr.td`로 표현할 수 있다.\n",
    "\n",
    "태그 t의 이름은 `t.name`으로 문자열로 된 값(`t.string`으로 원래 내용에 접근할 수 있고\n",
    "\n",
    "`t.stripped_string`을 쓰면 공백을 제거한 문자열 리스트를 반환한다)이 있다. \n",
    "\n",
    "부모 태그는 `t.parent`, 다음 태그는 `t.next`, 바로 전 태그는 `t.prev`이며, \n",
    "\n",
    "자식 태그(태그 안의 태그)는 `t.children`이다.\n",
    "\n",
    "BeautifulSoup 모듈에서는 파이썬 딕셔너리 인터페이스로 HTML 태그 속성에 접근할 수 있다. \n",
    "\n",
    "객체 t가 `<a href=\"foobar.html\">` 같은 하이퍼링크라면, \n",
    "    \n",
    "링크의 문자열 값은 `t[\"href\"].string`이 된다. HTML 태그는 대·소문자를 구분하지 않는다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup 모듈 다루기 (2)\n",
    "\n",
    "아마도 soup 함수 중 가장 유용한 함수는 `soup.find()`와 `soup.find_all()`일 것이다. \n",
    "\n",
    "특정한 태그의 첫 번째 인스턴스나 전체 인스턴스를 찾는 데 사용한다. \n",
    "\n",
    "몇 가지 사용 예를 살펴보자.\n",
    "\n",
    "◼︎ `<H2>` 태그로 된 모든 인스턴스\n",
    "\n",
    "`level2headers = soup.find_all(\"H2\")`\n",
    "\n",
    "◼︎ 볼드나 이탤릭 포맷으로 된 모든 인스턴스\n",
    "\n",
    "`formats = soup.find_all([\"i\", \"b\", \"em\", \"strong\"])`\n",
    "\n",
    "◼︎ 특정한 속성(id=\"link3\" 같은)을 가진 모든 태그\n",
    "\n",
    "`soup.find(id=\"link3\")`\n",
    "\n",
    "◼︎ 모든 하이퍼링크나 첫 번째 링크(딕셔너리 구문이나 tag.get() 함수 사용)\n",
    "\n",
    "`links = soup.find_all(\"a\")`\n",
    "\n",
    "`firstLink = links[0][\"href\"]`\n",
    "\n",
    "◼︎ 혹은\n",
    "\n",
    "`firstLink = links[0].get(\"href\")`\n",
    "\n",
    "마지막 예에서 사용한 두 표현 모두 속성이 존재하지 않는다면 오류가 발생한다. \n",
    "\n",
    "태그를 추출하기 전에 `tag.has_attr()` 함수를 사용해서 속성이 존재하는지 꼭 확인하자. \n",
    "\n",
    "다음 구문은 BeautifulSoup과 리스트 내포를 결합해 웹 페이지에 포함된 모든 링크와 \n",
    "\n",
    "그에 연결된 URL, 레이블을 추출한다(재귀적인 웹 크롤링(recursive web crawling)에 유용하다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(None, 'https://pragprog.com/book/dzpyds/data-science-essentials-in-python'),\n",
       " (None,\n",
       "  'https://pragprog.com/book/dzcnapy/complex-network-analysis-in-python'),\n",
       " ('DZPYDS', 'https://www.amazon.com/gp/product/1680501844'),\n",
       " ('DZCNAPY', 'https://www.amazon.com/gp/product/1680502697'),\n",
       " ('Networks of Music Groups as Success Predictors',\n",
       "  'http://www.slideshare.net/DmitryZinoviev/networks-of-music-groups-as-success-predictors'),\n",
       " ('Network Science Workshop',\n",
       "  'http://www.slideshare.net/DmitryZinoviev/workshop-20212296'),\n",
       " ('Resilience in Transaction-Oriented Networks',\n",
       "  'http://www.slideshare.net/DmitryZinoviev/resilience-in-transactional-networks'),\n",
       " ('Peer Ratings in Massive Online Social Networks',\n",
       "  'http://www.slideshare.net/DmitryZinoviev/peer-ratings-in-massive-online-social-networks'),\n",
       " ('Semantic Networks of Interests in Online NSSI Communities',\n",
       "  'http://www.slideshare.net/DmitryZinoviev/presentation-31680572'),\n",
       " ('Towards an Ideal Store',\n",
       "  'http://www.slideshare.net/DmitryZinoviev/10-monthsymposiumbeta'),\n",
       " ('D.Zinoviev, \"Analyzing Cultural Domains with Python,\"',\n",
       "  'https://media.pragprog.com/newsletters/2016-04-06.html'),\n",
       " ('D. Zinoviev, D. Stefanescu, G. Fireman, and L. Swenson, \"Semantic networks of interests in online non-suicidal self-injury communities,\"',\n",
       "  'http://dhj.sagepub.com/content/2/2055207616642118.full'),\n",
       " ('D.Zinoviev, \"The Pain of Complexity,\"',\n",
       "  'http://www.mitpressjournals.org/doi/abs/10.1162/LEON_a_01271#.VzOvwHUrKzc'),\n",
       " ('D.Zinoviev, Z.Zhu, and K.Li, \"Building mini-categories in product networks,\"',\n",
       "  'http://link.springer.com/chapter/10.1007/978-3-319-16112-9_18'),\n",
       " ('D.Zinoviev, H.Benbrahim, G.Meszoely, and D.Stefanescu, \"Mitigation of delayed management costs in transaction-oriented systems,\"',\n",
       "  'http://arxiv.org/abs/1409.6771'),\n",
       " ('D.Zinoviev, H.Benbrahim, G.Meszoely, and D.Stefanescu, \"Simulating resilience in transaction-oriented networks,\"',\n",
       "  'http://dl.acm.org/citation.cfm?id=2499974'),\n",
       " ('D.Zinoviev, D.Stefanescu, L.Swenson, and G.Fireman, \"Semantic networks of interests in online NSSI communities,\"',\n",
       "  'http://arxiv.org/abs/1206.5520'),\n",
       " ('D.Zinoviev and S.Llewelyn, \"Co-Evolution of Friendship and Publishing in Online Blogging Social Networks,\"',\n",
       "  'http://arxiv.org/abs/1401.6964'),\n",
       " ('D.Zinoviev, \"Information diffusion in social networks,\"',\n",
       "  'http://dl.acm.org/citation.cfm?id=2208181'),\n",
       " ('D.Zinoviev and V.Duong, \"A game theoretical approach to broadcast  information diffusion in social networks,\"',\n",
       "  'http://dl.acm.org/citation.cfm?id=2048377'),\n",
       " ('D.Zinoviev and V.Duong, \"A game theoretical approach to modeling full-duplex information dissemination,\"',\n",
       "  'http://dl.acm.org/citation.cfm?id=1999462'),\n",
       " ('D.Zinoviev, V.Duong, and H.Zhang, \"A game theoretical approach to modeling information dissemination in social networks,\"',\n",
       "  'http://arxiv.org/abs/1006.5493'),\n",
       " ('D.Zinoviev and V.Duong, \"Toward Understanding Friendship in Online Social Networks,\"',\n",
       "  'http://arxiv.org/abs/0902.4658'),\n",
       " ('D.Zinoviev, \"Topology and Geometry of Online Social Networks,\"',\n",
       "  'http://arxiv.org/abs/0807.3996'),\n",
       " ('Vixi: The Game of Meaning', '/vixi/html5/'),\n",
       " ('Evgenia Cherkasova', 'http://meaningoflife.cherkasova.org/'),\n",
       " ('All Characters from War and Peace by L.Tolstoy', 'v2.jpg'),\n",
       " ('Mapping the Bible: Social Networks in the Holy Book', 'bible-networks.pdf'),\n",
       " ('FIFA World Cup 2014: Who Beat Whom?', 'wc2014.gif'),\n",
       " ('The seed post \"9 American habits I lost when I moved to Germany\" and its 125 \"likes\" and \"shares\" on Facebook',\n",
       "  'facebook_spread.gif'),\n",
       " ('Email', 'mailto:dzinoviev@suffolk.edu'),\n",
       " ('Suffolk University',\n",
       "  'https://www.suffolk.edu/academics/faculty/z/i/dmitry-zinoviev'),\n",
       " ('Google Scholar',\n",
       "  'https://scholar.google.com/citations?hl=en&user=j5GjuIkAAAAJ&sortby=pubdate&view_op=list_works&pagesize=100'),\n",
       " ('LinkedIn', 'https://www.linkedin.com/pub/dmitry-zinoviev/4/a78/27b'),\n",
       " ('Academia.edu', 'https://suffolk.academia.edu/DmitryZinoviev'),\n",
       " ('ResearchGate', 'https://www.researchgate.net/profile/Dmitry_Zinoviev')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with urlopen(\"http://www.networksciencelab.com/\") as doc:\n",
    "    soup = BeautifulSoup(doc)\n",
    "\n",
    "links = [(link.string, link[\"href\"])\n",
    "for link in soup.find_all(\"a\")\n",
    "if link.has_attr(\"href\")]\n",
    "links # 튜플의 리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML/XML의 장점은 폭넓은 사용성이지만, 이는 단점이기도 하다. \n",
    "\n",
    "특히 테이블형 데이터를 다룰 때 그러하다. \n",
    "\n",
    "다행하게도 여러분은 테이블형 데이터를 안정적이고 쉽게 가공할 수 있는 CSV 파일에 저장할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON 파일 읽기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON은 간단한 데이터 교환 포맷이다. \n",
    "\n",
    "‘UNIT 12. pickle로 데이터 압축하기’에서 다루었던 pickle과는 달리 \n",
    "\n",
    "JSON은 사용하는 언어에 의존적이지는 않지만 \n",
    "\n",
    "데이터를 표현하는 데는 제약 사항이 더 많다.\n",
    "\n",
    "Twitter, Facebook이나 Yahoo! 날씨 같은 유명한 웹 사이트는\n",
    "\n",
    "데이터 교환 포맷으로 JSON을 사용한 API를 제공한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON의 자료 구조\n",
    "\n",
    "JSON은 다음 데이터 타입을 지원한다.\n",
    " \n",
    "◼︎ 기본 데이터 타입 : 문자열, 숫자, 참(true), 거짓(false), null\n",
    "\n",
    "◼︎ 배열(arrays) : 파이썬의 리스트와 같다. 배열은 대괄호(`[]`)로 씌워서 표현한다. \n",
    "\n",
    "배열 안의 아이템이 같은 데이터 타입일 필요는 없다.\n",
    "\n",
    "`[1, 3.14, \"a string\", true, null]`\n",
    "\n",
    "◼︎ 객체(objects) : 파이썬의 딕셔너리에 대응된다. 객체는 중괄호(`{}`)로 씌워서 표현한다. \n",
    "\n",
    "객체 안의 모든 아이템은 키(key)와 값(value)으로 구성되며, 쉼표로 구분한다.\n",
    "\n",
    "`{\"age\" : 37, \"gender\" : \"male\", \"married\" : true}`\n",
    " \n",
    "◼︎ 배열이나 객체, 기본 데이터 타입으로 구성된 어떤 재귀적인 조합\n",
    "\n",
    "(객체로 구성된 배열, 배열을 아이템 값으로 가지는 객체 등)\n",
    " \n",
    "◼︎ 안타깝게도 집합(sets)이나 복소수(complex number) 등\n",
    "\n",
    "몇몇 파이썬 데이터 타입과 구조는 JSON 파일에 저장할 수 없다.\n",
    "\n",
    "그러므로 이러한 타입을 다룰 때는 JSON으로 내보내기 전에 \n",
    "\n",
    "먼저 표현 가능한 데이터 타입으로 변형하는 작업을 해야 한다. \n",
    "\n",
    "복소수는 2개의 실수가 담긴 배열로 변환하고, 집합은 아이템의 배열로 저장할 수 있다.\n",
    "\n",
    "복잡한 데이터를 JSON 파일에 저장하는 것을 직렬화(serialization)라고 한다. \n",
    "\n",
    "그 반대는 역직렬화(deserialization)다. \n",
    "\n",
    "파이썬은 JSON 직렬화와 역직렬화를 json 모듈의 함수로 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON 모듈 다루기\n",
    "\n",
    "`dump()` 함수는 열려 있는 텍스트 파일에 파이썬 객체를 내보낸다(export). \n",
    "\n",
    "`dumps()` 함수는 파이썬 객체를 텍스트 문자열로 내보내는데, \n",
    "\n",
    "데이터를 읽기 쉽게 출력하거나 프로세스 간 커뮤니케이션을 하려는 목적에서 사용한다. \n",
    "\n",
    "`dump()`와 `dumps()` 함수 모두 JSON 직렬화를 수행한다.\n",
    "\n",
    "`loads()` 함수는 JSON 문자열을 파이썬 객체로 변환한다(객체를 파이썬으로 ‘불러온다’). \n",
    "\n",
    "이 변환은 언제나 가능하다. \n",
    "\n",
    "마찬가지로 load() 함수는 열려 있는 텍스트 파일에 담긴 내용을 파이썬 객체로 변환한다. \n",
    "\n",
    "하나의 JSON 파일에 2개 이상의 객체를 저장하면 오류가 발생한다. \n",
    "\n",
    "그러나 이미 있는 파일에 2개 이상의 객체가 있다면 \n",
    "\n",
    "이를 텍스트로 읽어서 텍스트를 객체의 배열로 변환한 다음\n",
    "\n",
    "(텍스트 주변에는 대괄호를, 객체 사이에는 쉼표 구분자를 달면 된다) \n",
    "\n",
    "`loads()` 함수를 사용해서 텍스트를 객체의 리스트로 역직렬화하면 오류가 발생하지 않는다.\n",
    "\n",
    "## JSON 모듈을 이용한 직렬화 및 역직렬화의 예시\n",
    "\n",
    "다음 코드는 (직렬화할 수 있는) 객체를 직렬화하고 역직렬화한다.\n",
    "\n",
    "네 번이나 고통스럽게 변환하는 과정을 거쳤지만 object, object1, object2는 여전히 모두 값이 같다.\n",
    "\n",
    "일반적으로 JSON 표현은 최종 결과물을 저장할 때 사용하는데, 여러분이 다른 프로그램으로 결과 값을 더 처리하거나 임포트해야 할 때 쓰면 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 어떤 직렬화 가능 객체를 만든다.\n",
    "object = [11,22,33,44,55]\n",
    "\n",
    "# 객체를 파일에 저장한다.\n",
    "with open(\"data.json\", \"w\") as out_json:\n",
    "    json.dump(object, out_json, indent=None, sort_keys=False)\n",
    "\n",
    "# 파일에서 객체를 읽어 온다.\n",
    "with open(\"data.json\") as in_json: object1 = json.load(in_json)\n",
    "\n",
    "# 객체를 문자열로 직렬화한다.\n",
    "json_string = json.dumps(object1)\n",
    "\n",
    "# 문자열을 JSON으로 파싱한다.\n",
    "object2 = json.loads(json_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어 처리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경험에 비추어 보았을 때, 사용 가능한 모든 데이터의 80% 가량은 비정형적이다. \n",
    "\n",
    "비정형 데이터에는 소리, 영상, 이미지(이 책에서는 다루지 않는다)와 자연어로 된 텍스트가 있다. \n",
    "\n",
    "자연어로 된 텍스트에는 태그, 구분자, 데이터 타입도 없지만, 풍부한 정보를 담고 있을 수 있다. \n",
    "\n",
    "자연어 텍스트를 분석해서 특정 단어를 사용했는지, 얼마나 자주 사용했는지, \n",
    "\n",
    "어떤 종류의 텍스트인지(텍스트 분류), 긍정적이거나 부정적인 메시지를 담고 있는지(감성 분석), \n",
    "\n",
    "누가 혹은 무엇을 언급했는지(내용 추출) 등 다양한 분야의 정보를 얻을 수 있다. \n",
    "\n",
    "1~2개의 텍스트는 직접 읽을 수 있지만, \n",
    "\n",
    "대규모의 텍스트 분석은 자동화된 자연어 처리(NLP, Natural Language Processing)가 필요하다.\n",
    "\n",
    "상당수 NLP 기능은 파이썬의 nltk(natural language toolkit) 모듈에 구현되어 있다. \n",
    "\n",
    "이 모듈은 코퍼스, 함수와 알고리즘으로 구성된다.\n",
    "\n",
    "nltk 모듈을 설치하면 코퍼스가 아니라 클래스만 설치한다. \n",
    "\n",
    "배포에 포함하기에는 코퍼스 크기가 너무 크기 때문이다.\n",
    "\n",
    "따라서 최초로 모듈을 임포트할 때는 download() 함수를 실행해야 한다는 것을 기억하자(인터넷 연결이 필요하다). \n",
    "\n",
    "그리고 상황에 따라서 필요한 부분을 추가로 설치한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코퍼스\n",
    "\n",
    "코퍼스(corpus)(말뭉치)는 정형이나 비정형인 단어나 표현의 묶음이다. \n",
    "\n",
    "모든 NLTK 코퍼스는 nltk.corpus 모듈에 저장되어 있다. 예를 들면 다음과 같다.\n",
    "\n",
    "◼︎ gutenberg : <모비딕(Moby Dick)>이나 <성경> 등 \n",
    "\n",
    "구텐베르크 프로젝트(Gutenberg Project)에서 제공하는 영문 텍스트 18개\n",
    "\n",
    "◼︎ names : 8000개의 남성과 여성의 이름 리스트\n",
    "\n",
    "◼︎ words : 가장 빈번하게 사용하는 영어 단어 23만 5000개\n",
    "\n",
    "◼︎ stopwords : 14개의 언어로 된 가장 많이 사용하는 불용어(stop word) 리스트. \n",
    "\n",
    "영어로 된 리스트는 stop words.words(\"english\")에 저장되어 있다. \n",
    "\n",
    "불용어는 대부분의 분석에서 보통 삭제하는데, 텍스트 이해에 별로 기여하는 바가 없기 때문이다.\n",
    "\n",
    "◼︎ cmudict : 카네기멜론대학교에서 만든 발음 사전으로 13만 4000개 입력 데이터가 있다. \n",
    "\n",
    "`cmudict.entries()`의 각 입력 데이터는 단어와 그 음절(syllables) 리스트의 튜플이다. \n",
    "\n",
    "단어가 같더라도 다르게 발음할 수 있다. \n",
    "\n",
    "이 코퍼스를 사용하면 발음이 같은 동음이의어(homophones)를 찾아볼 수 있다.\n",
    "\n",
    "`nltk.corpus.wordnet` 객체는 온라인에 구축된 의미론적(semantic) 단어 네트워크인\n",
    "\n",
    "Wordnet에 접근하는 인터페이스다(사용하려면 인터넷에 연결해야 한다). \n",
    "\n",
    "이 네트워크는 synsets(유의어 묶음)로 구성되어 있고,\n",
    "\n",
    "각 synset은 단어와 품사, 순번으로 구성되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('cat.n.01'),\n",
       " Synset('guy.n.01'),\n",
       " Synset('cat.n.03'),\n",
       " Synset('kat.n.01'),\n",
       " Synset('cat-o'-nine-tails.n.01'),\n",
       " Synset('caterpillar.n.02'),\n",
       " Synset('big_cat.n.01'),\n",
       " Synset('computerized_tomography.n.01'),\n",
       " Synset('cat.v.01'),\n",
       " Synset('vomit.v.01')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download() # 첫 실행시 다운로드\n",
    "wn = nltk.corpus.wordnet # 코퍼스 리더(reader)\n",
    "wn.synsets(\"cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "synset은 상위어(hypernyms)와 하위어(hyponyms)를 가질 수 있는데, \n",
    "\n",
    "이러한 특징은 synset을 하위 클래스(subclass)와 상위 클래스(superclass)를 가진 \n",
    "\n",
    "OOP(객체 지향 프로그래밍) 클래스처럼 보이게 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('domestic_cat.n.01'), Synset('wildcat.n.03')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset(\"cat.n.01\").hypernyms()\n",
    "wn.synset(\"cat.n.01\").hyponyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 여러분은 WordNet을 사용해서 두 synset 간 의미론적 유사도를 계산할 수 있다. \n",
    "\n",
    "유사도는 0에서 1 사이 실수다. \n",
    "\n",
    "유사도가 0이면 두 단어는 서로 관계가 없지만, 유사도가 1이라면 완전한 유의어다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = wn.synset(\"cat.n.01\")\n",
    "y = wn.synset(\"lynx.n.01\")\n",
    "x.path_similarity(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러면 임의의 두 단어는 서로 얼마나 가까울까? \n",
    "\n",
    "‘dog’와 ‘cat’의 모든 synset을 살펴보고, 가장 의미론적으로 가까운 정의를 찾아보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an informal term for a youth or man', 'informal term for a man']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[simxy.definition() for simxy in max(\n",
    "  (x.path_similarity(y), x, y)\n",
    "  for x in wn.synsets('cat')\n",
    "  for y in wn.synsets('dog')\n",
    "  if x.path_similarity(y) # synset들이 서로 관련 있는지 확인한다.\n",
    ")[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 나만의 코퍼스 만들기\n",
    "\n",
    "기본적인 코퍼스 외에도 PlaintextCorpusReader로 여러분만의 코퍼스를 만들 수 있다. \n",
    "\n",
    "리더는 root 디렉터리 경로에서 glob 패턴과 일치하는 파일을 찾는다.\n",
    "\n",
    "`myCorpus = nltk.corpus.PlaintextCorpusReader(root, glob)`\n",
    "\n",
    "`fileids()` 함수는 새롭게 만든 코퍼스에 포함된 파일 리스트를 반환한다.\n",
    "\n",
    "`raw()` 함수는 코퍼스에 있는 ‘원천(raw)’ 텍스트를 반환한다. \n",
    "\n",
    "`sents()` 함수는 모든 문장을 리스트로 반환한다. \n",
    "\n",
    "`words()` 함수는 모든 단어를 리스트 안에 넣어 반환한다.\n",
    "\n",
    "Counter 객체와 함께 사용하면 단어 빈도를 계산하고 등장 빈도가 가장 높은 단어를 뽑을 수 있다.\n",
    "\n",
    "이어지는 내용에서 원천 텍스트를 문장과 단어로 변환하는 마법이 어떻게 일어나는지 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정규화\n",
    "\n",
    "정규화(normalization)는 추가적으로 데이터를 처리하려고 자연어로 된 텍스트를 준비하는 과정이다. \n",
    "\n",
    "이는 전형적으로 다음 단계로 진행한다(보통 이러한 순서를 따른다).\n",
    "\n",
    "### 토큰화(tokenization)\n",
    "\n",
    "이 단계에서는 텍스트를 단어로 쪼갠다.\n",
    "\n",
    "NLTK는 간단한 버전의 토크나이저 2개, 고급 버전 2개를 제공한다. \n",
    "\n",
    "문장 토크나이저는 문자열로 된 문장 리스트를 반환한다. 나머지 토크나이저는 단어 리스트를 반환한다.\n",
    "\n",
    "□ word_tokenize(text) : 단어 토크나이저\n",
    "\n",
    "□ sent_tokenize(text) : 문장 토크나이저\n",
    "\n",
    "□ regexp_tokenize(text, re) : 정규 표현식 기반의 토크나이저. re 파라미터는 단어를 표현하는 정규 표현식\n",
    " \n",
    "토크나이저의 퀄리티와 문장 구조에 따라서 어떤 단어는 알파벳이 아닌 문자를 포함할 수도 있다. \n",
    "\n",
    "이모티콘을 이용한 감성 분석 등 문장 구조를 깊이 분석하는 작업을 할 때는 \n",
    "\n",
    "WordPunctTokenizer 같은 고도화된 도구가 필요하다. \n",
    "\n",
    "같은 텍스트를 WordPunctTokenizer.tokenize()와 word_tokenize()가 어떻게 파싱하는지 비교해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['}', 'Help', '!', ':)))', ':[', '.....', ':', 'D', '{']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "word_punct = WordPunctTokenizer()\n",
    "text = \"}Help! :))) :[ ..... :D{\"\n",
    "word_punct.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['}', 'Help', '!', ':', ')', ')', ')', ':', '[', '...', '..', ':', 'D', '{']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"}Help! :))) :[ ..... :D{\"\n",
    "nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어의 대·소문자를 통일한다.\n",
    "\n",
    "### 불용어를 제거한다.\n",
    "\n",
    "stopwords 코퍼스와 부가적으로 작업에 필요한 불용어 리스트를 참조한다. \n",
    "\n",
    "stopwords에 있는 단어는 모두 소문자로 되어 있다는 것을 기억하자. \n",
    "\n",
    "“THE”(불용어가 확실하다)는 코퍼스에서 찾을 수 없을 것이다.\n",
    "\n",
    "### 형태소 분석(stemming)\n",
    "\n",
    "단어를 형태소로 변환한다.\n",
    "\n",
    "NLTK는 2개의 기본 형태소 분석기를 제공한다. \n",
    "\n",
    "포터(Porter) 형태소 분석기는 보수적이고, \n",
    "\n",
    "랭커스터(Lancaster) 형태소 분석기는 더 적극적(aggressive)이다. \n",
    "\n",
    "형태소 분석 규칙의 적극성 때문에 \n",
    "\n",
    "랭커스터 형태소 분석기는 더 많은 동음이의어 형태소(homonymous stem)를 생산한다. \n",
    "\n",
    "두 분석기 모두 단어의 형태소를 반환하는 stem(word) 함수가 있다.\n",
    "\n",
    "전체 문장이 아니라 단어 하나에서 형태소 분석기를 사용해야 한다. 그래야 제대로 작동한다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wonder'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pstemmer = nltk.PorterStemmer()\n",
    "pstemmer.stem(\"wonderful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wond'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstemmer = nltk.LancasterStemmer()\n",
    "lstemmer.stem(\"wonderful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원형 추출(lemmatization)\n",
    "\n",
    "더 느리고 더 보수적인 형태소 추출 메커니즘이다. \n",
    "\n",
    "WordNetLemmatizer는 WordNet이 계산한 형태소를 참조해 \n",
    "\n",
    "문장에서 단어나 표현을 인식한다(원형 추출기를 사용하려면 인터넷에 연결해야 한다). \n",
    "\n",
    "`lemmatize(word)` 함수는 단어의 원형을 반환한다.\n",
    "\n",
    "\n",
    "정규화 과정의 일부는 아니지만, 품사 태깅(POS tagging)은 텍스트 처리에서 매우 중요한 단계다. \n",
    "\n",
    "`nltk.pos_tag(text)`는 텍스트(단어의 리스트)에 있는 모든 단어에 품사를 할당한다. \n",
    "\n",
    "반환되는 값은 튜플의 리스트인데, 튜플의 첫 번째 요소는 원래 단어고 두 번째 요소는 품사다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wonderful'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(\"wonderful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('beautiful', 'JJ'), ('world', 'NN')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag([\"beautiful\", \"world\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 다룬 모든 것을 이어 붙여 \n",
    "\n",
    "index.html 파일에서 (불용어를 제외하고) 가장 많이 등장한 단어 원형을 찾아보자\n",
    "\n",
    "(‘UNIT 13. HTML 파일 처리하기’에서 다룬 BeautifulSoup을 사용하면 된다).\n",
    "\n",
    "이러한 코드는 주제 추출(topic extraction)의 첫 번째 단계라고도 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('h', 1), ('e', 1), ('l', 1), ('p', 1), ('d', 1)]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import LancasterStemmer\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# 형태소 분류기를 생성한다.\n",
    "ls = nltk.LancasterStemmer()\n",
    "\n",
    "# 파일을 읽고 soup을 만든다.\n",
    "soup = BeautifulSoup(urlopen(\"http://www.networksciencelab.com/\"))\n",
    "\n",
    "# 텍스트를 추출하고 토큰화한다.\n",
    "words = nltk.word_tokenize(soup.text)\n",
    "\n",
    "# 단어를 소문자로 변환한다.\n",
    "words = [w.lower() for w in words]\n",
    "\n",
    "# 불용어를 제거하고 단어의 형태소를 추출한다.\n",
    "words = [ls.stem(w) for w in text if w not in stopwords.words(\"english\")\n",
    "         and w.isalnum()]\n",
    "\n",
    "# 가장 빈번하게 등장한 10개의 단어를 추출한다.\n",
    "freqs = Counter(words)\n",
    "print(freqs.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다른 텍스트 처리 방식\n",
    "\n",
    "고급 NLP 방법을 논의하는 것은 이 책의 범위를 벗어나지만, \n",
    "\n",
    "여러분의 흥미를 돋우려고 몇 가지 옵션을 간단히 알아보겠다.\n",
    "\n",
    " \n",
    "◼︎ 세그먼테이션(segmentation)\n",
    "\n",
    "중국어처럼 단어 사이에 구문적 경계가 없는 텍스트에서 단어 경계를 인식하는 기법이다. \n",
    "\n",
    "세그먼테이션은 연속적인 문자나 숫자에도 적용할 수 있다(예를 들어 연속적인 구매 기록이나 DNA 파편 등).\n",
    "\n",
    "◼︎ 텍스트 분류(text classification)\n",
    "\n",
    "카테고리와 분류 기준을 설정하고 텍스트를 분류한다.\n",
    "\n",
    "텍스트 분류의 대표적인 예는 감성 분석으로 일반적으로 감정이 담긴 단어의 빈도를 기반으로 분류한다.\n",
    "\n",
    "◼︎ 대상 추출(entity extraction)\n",
    "\n",
    "설정 값에 부합하는 단어나 구문을 탐지하는데, 보통 인명, 지명, 법인 이름, 제품 이름이나 브랜드 등을 대상으로 한다.\n",
    "\n",
    "◼︎ 잠재적 의미 색인(latent semantic indexing)\n",
    "\n",
    "특이 값 분해(SVD, Singular Value Decomposition)를 사용해 \n",
    "\n",
    "비정형 텍스트 뭉치에서 등장하는 표현과 콘셉트 간의 관계를 규명한다.\n",
    "\n",
    "SVD는 통계학에서는 주성분 분석(PCA, Principal Component Analysis)으로 알려져 있다.\n",
    " \n",
    "◼︎ 자연어: 사람들이 일상적으로 쓰는 언어로, 인공적으로 만든 언어와 구분해 부르는 개념\n",
    "\n",
    "◼︎ 자연어 처리: 사람들이 쓰는 보통 언어를 컴퓨터에 인식시켜서 처리하는 일\n",
    "\n",
    "◼︎ 불용어: 제외어라고도 하며, 색인 작성이나 인터넷 검색 등에 사용하지 않는 언어\n",
    "\n",
    "◼︎ 특이 값 분해: 선형대수에서 실수나 복소수 행렬의 인수분해를 말하는 것으로, 행렬의 역행렬을 잘 구할 수 없을 때 유용\n",
    "\n",
    "◼︎ 주성분 분석: 통계 데이터를 분석하는 하나의 기법으로 고차원의 데이터를 저차원의 데이터로 환원시킴. \n",
    "\n",
    "예를 들어 어떤 개체를 설명하는데 x종의 데이터가 있다고 한다면 x종을 가장 적은 특성으로 정리하는 기법\n",
    "\n",
    "# 연습 문제\n",
    "\n",
    "이쯤 되면 여러분은 HTML, XML, CSV나 JSON 파일, 플레인 텍스트에서\n",
    "\n",
    "귀중한 데이터를 추출하는 방법을 터득했을 것이다. \n",
    "\n",
    "HTML, XML 태그와 그 구조를 이해하고, 데이터에서 태그를 분리하며, \n",
    "\n",
    "(어느 정도) 단어를 정규화하는 방법을 배웠다. \n",
    "\n",
    "지금까지 배운 것을 활용할 수 있고, 약간의 인내심이 필요한 연습문제들이 기다리고 있다. 도전해 보자.\n",
    "\n",
    "## 끊어진 링크 탐지기(Broken Link Detector) ★☆☆\n",
    "\n",
    "웹 페이지의 URL을 입력받아 해당 웹 페이지에서 연결이 끊긴 링크 이름과 연결 대상을 출력하는 프로그램을 작성해 보자. \n",
    "\n",
    "연습문제 목적에 따라 urllib.request.urlopen()으로 URL을 열 때 오류가 발생한다면 링크가 끊긴 것으로 인식한다.\n",
    "\n",
    "## 위키피디아 마이너(Wikipedia Miner) ★★☆\n",
    "\n",
    "미디어위키(MediaWiki)는 위키피디아 데이터와 메타데이터에 접근할 수 있는 JSON 기반 API를 제공한다. \n",
    "\n",
    "제목이 ‘Data science’인 위키피디아 페이지에서 가장 많이 사용한 형태소를 출력하는 프로그램을 작성해 보자.\n",
    "\n",
    "구현 힌트\n",
    "\n",
    "◼︎ HTTPS가 아니라 HTTP를 사용한다.\n",
    "\n",
    "◼︎ 미디어위키에서 ‘simple example’을 읽어 보고, 작성할 프로그램의 기반으로 사용한다.\n",
    "\n",
    "◼︎ 먼저 제목으로 페이지 ID를 얻은 후 그 ID로 페이지에 접근한다.\n",
    "\n",
    "◼︎ JSON 데이터를 시각적으로 살펴본다. \n",
    "\n",
    "특히 데이터의 상하 구조에서 사용된 키(key)를 눈여겨보자. \n",
    "\n",
    "이 글을 쓰는 시점에서 답은 여섯 번째 하위 항목에 있다.\n",
    " \n",
    "## 음악 장르 분류기(Music Genre Classifier) ★★★\n",
    "\n",
    "위키피디아를 사용해서 록(rock)과 팝(pop) 음악 장르 간 의미론적 유사도를 계산하는 프로그램을 작성해 보자. \n",
    "\n",
    "장르별 주요 음악 그룹 리스트로 시작해 보자(리스트에는 위계가 있으며 하위 카테고리가 존재한다). \n",
    "\n",
    "관련된 모든 그룹을 찾을 때까지 리스트와 하위 항목을 재귀적으로 처리하자\n",
    "\n",
    "(시간과 트래픽을 아끼려고 영국 록 그룹처럼 탐색 범위를 좁혀도 된다). \n",
    "\n",
    "결과로 얻은 각 그룹에서 (가능하다면) 장르를 추출해 보자. \n",
    "\n",
    "자카드(Jaccard) 유사도 인덱스를 사용해서 의미론적 유사도를 계산하자. \n",
    "\n",
    "장르 A와 B의 쌍에서 J(A,B)=|A∩B|/|A∪B|=|C|/(|A|+|B|-|C|)인데 |A|와 |B|는 각 장르에 속하는 그룹 개수이며, \n",
    "\n",
    "|C|는 A와 B에 모두 속하는 그룹 개수다. \n",
    "\n",
    "결과를 pickle로 저장해 차후에 또 쓸 수 있게 하자. 이 프로그램을 두 번 다시 돌려 보고 싶지 않을 것이다.\n",
    "\n",
    "전체적으로 몇 개의 장르가 있으며, 어떤 장르가 서로 가장 강하게 연결되어 있는가?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
